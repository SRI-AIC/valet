from typing import Dict, List, Generator, Tuple, Type

from .fssrc import FileSource, DirectorySource, DirectoryTreeSource
from .sentencer import Sentencer
from .tokenizer import PlainTextTokenizer, TokenSequence


# Historically the requirements were stored in the tokenizer.
# Bob feels they're probably better in the tseqsrc (or the tseqs?),
# e.g., when the tokenization is determined by the file format.
# But for now we'll use a dummy tokenizer instance in the
# now non-tokenizing base class.
class DummyTokenizer(object):
    def __init__(self):
        self.requirements = set()

    def add_requirements(self, reqs):
        self.requirements |= reqs

    def set_requirements(self, reqs):
        self.requirements = reqs


class TokenSequenceSource(object):
    """
    Key abstract method is token_sequences, which generates tuples
    with a source name string and a list of token sequences.

    Most typically, a token sequence corresponds to a sentence and
    source corresponds to a document (which often corresponds to a file),
    but all this is determined by the implementing subclass type
    and can vary.

    Also provides class methods for registering subclasses (or not) with
    an associated short string (e.g., specified as a command line argument) and
    constructing instances of the class associated with the short string value.
    Provides a small set of pre-registered subclasses.
    """

    # Fill in (replace) at end of file after classes are defined.
    source_types: Dict[str, Type['TokenSequenceSource']] = {}

    @classmethod
    def source_for_type(cls, type_, source_name,
                        aux_file=None, positive_label=None,
                        rewrites=None, nlp_engine='stanza',
                        **kwargs):
        """Look up TokenSequenceSource (sub)class by its associated
        short string. Throws KeyError if not found."""
        tsrc_cls = cls.source_types[type_]
        return tsrc_cls(source_name,
                        aux_file=aux_file, positive_label=positive_label,
                        rewrites=rewrites, nlp_engine=nlp_engine,
                        **kwargs)

    @classmethod
    def add_token_sequence_source(cls, name, src):
        cls.source_types[name] = src

    @classmethod
    def available_type_labels(cls):
        return cls.source_types.keys()

    @classmethod
    def register(cls, child_cls):
        cls.add_token_sequence_source(child_cls.NAME, child_cls)

    def __init__(self, source_name,
                 aux_file=None,
                 positive_label=None,
                 rewrites=None,
                 nlp_engine='stanza',
                 **kwargs
                 ):
        self.source_name = source_name
        self.aux_file = aux_file
        self.positive_label = positive_label
        self.rewrites = rewrites
        self.nlp_engine = nlp_engine
        self.tokenizer = DummyTokenizer()

    def set_requirements(self, requirements):
        self.tokenizer.set_requirements(requirements)

    # TODO? Make a variable for this return type?
    def token_sequences(self) -> Generator[Tuple[str, List[TokenSequence]], None, None]:
        """The usual intention seems to be that each item will represent
        a document and each TokenSequence will represent a sentence."""
        raise NotImplementedError()

    # TODO? Not sure whether this gets used or not. I think not.
    # The meaning of this is not described anywhere, but the intention seems
    # to be that it is the number of items that will be generated by the
    # token_sequences method.
    # The usual intention seems to be that each item should be a document
    # and each TokenSequence should be a sentence.
    # In some cases, it can be hard to determine without generating all
    # the token_sequences.
    # For example, with CONLL files, there can be multiple documents
    # per file, so the file count (which is currently returned) is not
    # the same as the document count.
    # I'm inclined to drop this method until we know there's a user for it.
    # Currently, not all TSS's implement it correctly, or at all.
    def __len__(self):
        """Should probably be the number of items generated by token_sequences,
        currently isn't always. Might not be currently used"""
        raise NotImplementedError()


# Still abstract, as it does not implement token_sequences.
class PlainTokenSequenceSource(TokenSequenceSource):
    """
    Abstract subclass providing sentencing and tokenization capabilities
    for plain text.

    Key implemented method is token_sequences_from_text, which returns
    a list of TokenSequence (or subclass) instances, where the text
    is first segmented into sentences, then tokenized using a provided
    tokenizer, and one TokenSequence is returned per sentence.
    """

    def __init__(self, source_name,
                 token_regex=r'[a-z]+|[0-9]+|\S|\n|[ \t]+',
                 skip_initial_regex=r'[^a-zA-Z]+',
                 **kwargs):
        super().__init__(source_name, **kwargs)
        self.token_regex = token_regex
        self.skip_initial_regex = skip_initial_regex
        self.tokenizer = self.get_tokenizer()
        self.sentencer = self.get_sentencer()

    def get_tokenizer(self):
        # TODO? There's a gratuitous name change here (engine->on_demand); should we try to unify them?
        return PlainTextTokenizer(preserve_case=True, token_regex=self.token_regex, nlp_on_demand=self.nlp_engine)

    def get_sentencer(self):
        return Sentencer(
            blank_line_terminals=True, tokenizer=self.tokenizer, skip_initial_regex=self.skip_initial_regex)

    def token_sequence_from_text(self, text) -> TokenSequence:
        """Tokenizes the text but does not break into sentences."""
        return self.tokenizer.tokens(text)

    def token_sequences_from_text(self, text) -> List[TokenSequence]:
        """Return list of TokenSequence (or subclass) instances,
        one for each sentence as determined by self.sentencer."""
        result = []
        for sentence in self.sentencer.sentences(text):
            toks = sentence.tokens()
            result.append(toks)
        return result


class PlainLinesFileSource(PlainTokenSequenceSource):
    """Applies parent class's self.token_sequences_from_text method
    to lines from a single text file specified by self.source_name.
    Hence each line is a source and each tseq is a sentence from
    that line."""

    def token_sequences(self) -> Generator[Tuple[str, List[TokenSequence]], None, None]:
        """Generates a tuple for each line, with source name a combination of
        self.source_name and a line index, and a list with token sequences
        for the sentences on the line."""
        fname = self.source_name
        with open(fname, "r") as fh:
            text = fh.read()
        count = 0
        for line in text.splitlines():
            if line == '':
                continue
            tseqs = self.token_sequences_from_text(line)
            if len(tseqs) > 0:
                yield "%s:%d" % (fname, count), tseqs
            count += 1

    def __len__(self):
        return 1  # TODO?


class PlainFileSource(PlainTokenSequenceSource):
    """Applies parent class's self.token_sequences_from_text method
    to a single text file specified by self.source_name.
    Hence there is just one source and the tseqs are all the sentences
    in the file."""

    NAME = 'text'

    def __init__(self, source_name, *, encoding="utf-8", **kwargs):
        super().__init__(source_name, **kwargs)
        # Most kwargs are probably intended for TokenSequenceSource,
        # but some might be for the fssrc.
        # Let each decide what applies to it.
        self.fssrc = FileSource(source_name, encoding=encoding, **kwargs)

    def token_sequences(self):
        """Generates a single tuple with self.source_name and a list of
        TokenSequence."""

        for path, text in self.fssrc.texts():
            yield str(path), self.token_sequences_from_text(text)

    # Note FWIW that this returns the number of items generated by token_sequences.
    # Same is true for the PlainDirectory[Tree]Source subclasses.
    def __len__(self):
        return len(self.fssrc)


class StringTextSource(PlainTokenSequenceSource):
    """Applies parent class's self.token_sequences_from_text method
    to a single in-memory string, which is specified by self.source_name.
    Hence there is just one source and the tseqs are all the sentences
    in the string.
    Contrast with PlainFileSource, which gets its text from a text file."""

    NAME = 'string'

    def __init__(self, source_name, real_source_name=None, **kwargs):
        super().__init__(source_name, **kwargs)
        if real_source_name is None:
            self.real_source_name = "String({})".format(source_name[0:50])

    def token_sequences(self):
        """Generates a single tuple with self.real_source_name and a list of
        TokenSequence."""
        text = self.source_name
        yield self.real_source_name, self.token_sequences_from_text(text)

    def __len__(self):
        return 1


class JsonLFileSource(PlainTokenSequenceSource):
    """Applies parent class's self.token_sequences_from_text method
    to a single column/field/key of a single jsonl file specified by self.source_name.
    Each text string in the column is a source, and the tseqs are all the
    sentences in the text."""

    NAME = 'jsonl'

    # Could add additional args to pass to csv.reader() if more control
    # is desired.
    def __init__(self, source_name, text_field="text", id_fields=[], **kwargs):
        """text_field: field to read from
        id_fields: fields to draw id of record from
        """
        super().__init__(source_name, **kwargs)
        self.text_field = text_field
        self.id_fields = id_fields

    def token_sequences(self):
        """Generates a single tuple with self.source_name and a list of
        TokenSequence."""
        import json
        fname = self.source_name
        with open(fname, "r") as fh:
            linenum = 0
            for line in fh:
                linej = json.loads(line)
                text = linej[self.text_field]
                lineid = "|".join([linej[x] for x in self.id_fields]) if self.id_fields else linenum
                tseqs = self.token_sequences_from_text(text)
                if len(tseqs) > 0:
                    yield "%s:%s" % (fname, lineid), tseqs
                linenum += 1

    # Probably not too important to support this.
    # We can implement it if someone wants it.
    def __len__(self):
        raise NotImplementedError()


class CsvFileSource(PlainTokenSequenceSource):
    """Applies parent class's self.token_sequences_from_text method
    to a single column of a single CSV file specified by self.source_name.
    Each cell in the column is a source, and the tseqs are all the sentences
    in the cell."""

    NAME = 'csv'

    # Could add additional args to pass to csv.reader() if more control
    # is desired.
    def __init__(self, source_name, column_header=None, **kwargs):
        """column_header: header string of column to read from"""
        super().__init__(source_name, **kwargs)
        self.column_header = column_header

    def token_sequences(self):
        """Generates a single tuple with self.source_name and a list of
        TokenSequence."""
        import csv
        fname = self.source_name
        with open(fname, "r") as fh:
            reader = csv.reader(fh)
            column_idx = None
            for rowi, row in enumerate(reader):
                if column_idx is None:
                    column_idx = row.index(self.column_header)
                    if column_idx < 0:
                        raise ValueError("No column named '%s' in header row '%s'" % (self.column_header, row))
                    continue
                text = row[column_idx]
                tseqs = self.token_sequences_from_text(text)
                if len(tseqs) > 0:
                    yield "%s:%d" % (fname, rowi), tseqs

    # Probably not too important to support this.
    # None of the CSV classes in projectsrc do.
    # We can implement it if someone wants it.
    def __len__(self):
        raise NotImplementedError()


class PlainDirectorySource(PlainFileSource):
    """Applies parent class's self.token_sequences_from_text method
    to the (assumed plain text) files in the self.source_name directory.
    Hence each file found is a source, and the tseqs are all the sentences
    in the the file."""

    NAME = 'directory'

    def __init__(self, source_name, *, encoding="utf-8", filter_regex=None, **kwargs):
        super().__init__(source_name, **kwargs)
        self.fssrc = DirectorySource(source_name, encoding=encoding, filter_regex=filter_regex, **kwargs)


class PlainDirectoryTreeSource(PlainDirectorySource):
    """Like the parent class but also recurses into subdirectories.
    Hence each file found is a source, and the tseqs are all the sentences
    in the the file."""

    NAME = 'dirtree'

    def __init__(self, source_name, *, encoding="utf-8", filter_regex=None, **kwargs):
        super().__init__(source_name, **kwargs)
        self.fssrc = DirectoryTreeSource(source_name, encoding=encoding, filter_regex=filter_regex, **kwargs)


# Predefined source type mapping.
TokenSequenceSource.source_types = dict(
    text=PlainFileSource,
    lines=PlainLinesFileSource,
    string=StringTextSource,
    csv=CsvFileSource,
    jsonl=JsonLFileSource,
    directory=PlainDirectorySource,
    dirtree=PlainDirectoryTreeSource
)
